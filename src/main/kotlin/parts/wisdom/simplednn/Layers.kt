/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package parts.wisdom.simplednn

import koma.extensions.*
import koma.internal.default.generated.ndarray.DefaultGenericNDArray
import koma.matrix.Matrix
import koma.ndarray.NDArray
import koma.rand
import koma.zeros
import kotlin.math.pow

private const val LEARNING_RATE = 3e-3
private const val MAX_INITIAL_VALUE = 1.0

abstract class HiddenLayer(
    val inputShape: Shape,
    val outputShape: Shape
) {
    /**
     * Compute this `Layer`'s output from its input.
     */
    abstract operator fun invoke(input: Matrix<Double>): Matrix<Double>

    /**
     * Make a stateful trainer that will process one batch of inputs.
     */
    abstract fun makeBatchTrainer(): HiddenLayerBatchTrainer
}

interface HiddenLayerBatchTrainer {
    // TODO: Refactor `train` to free up that name and use it for this function instead.
    fun train(
        input: Matrix<Double>,
        dLossDOutput: Matrix<Double>
    )
    fun updateParameters()
}

abstract class OutputLayer(
    val inputShape: Shape,
    val outputShape: Shape
) {
    /**
     * Compute this `Layer`'s output from its input.
     */
    abstract operator fun invoke(input: Matrix<Double>): Matrix<Double>

    /**
     * Make a stateful trainer that will process one batch of inputs.
     */
    abstract fun makeBatchTrainer(): OutputLayerBatchTrainer
}

interface OutputLayerBatchTrainer {
    fun train(
        input: Matrix<Double>,
        label: Coords
    )
    fun updateParameters()
}

/**
 * A linear fully connected layer (no activation function).
 */
class FullyConnected(
    inputShape: Shape,
    outputShape: Shape
) : HiddenLayer(inputShape, outputShape) {
    /**
     * For each output, a weight for each input.
     */
    private var weight: NDArray<Matrix<Double>> =
        makeArrayOfMatrices(outputShape) { _, _ ->
            rand(inputShape.numRows, inputShape.numCols) * MAX_INITIAL_VALUE
        }

    /**
     * A bias for each output.
     */
    private var bias: Matrix<Double> = rand(outputShape) * MAX_INITIAL_VALUE

    override operator fun invoke(input: Matrix<Double>): Matrix<Double> {
        val weightedSums =
            Matrix(
                outputShape.numRows,
                outputShape.numCols
            ) { row, col ->
                weight[row, col] dot input
            }
        return weightedSums + bias
    }

    inner class MyBatchTrainer: HiddenLayerBatchTrainer {
        private val batchDeltaBias =
            zeros(
                outputShape.numRows,
                outputShape.numCols
            )
        private val batchDeltaWeight: NDArray<Matrix<Double>> =
            makeArrayOfMatrices(outputShape) { _, _ ->
                zeros(inputShape.numRows, inputShape.numCols)
            }

        private var training = true

        override fun train(input: Matrix<Double>, dLossDOutput: Matrix<Double>) {
            check(training)
            dLossDOutput.forEachIndexedN { id, d ->
                val deltaBias = -LEARNING_RATE * d
                batchDeltaBias[id[0], id[1]] += deltaBias

                val dw = batchDeltaWeight[id[0], id[1]]
                for (row in 0 until inputShape.numRows) {
                    for (col in 0 until inputShape.numCols) {
                        val dLossDW = d * input[row, col]
                        val deltaW = -LEARNING_RATE * dLossDW
                        dw[row, col] += deltaW
                    }
                }
            }
        }

        override fun updateParameters() {
            bias += batchDeltaBias
            weight.forEachIndexedN { iw, _ ->
                weight[iw[0], iw[1]] += batchDeltaWeight[iw[0], iw[1]]
            }
            training = false
        }
    }

    override fun makeBatchTrainer(): HiddenLayerBatchTrainer =
        MyBatchTrainer()
}

/**
 * Softmax classifier with cross-entropy loss.
 */
class FullyConnectedSoftmax(
    inputShape: Shape,
    outputShape: Shape
) : OutputLayer(inputShape, outputShape) {
    val fullyConnected = FullyConnected(inputShape, outputShape)

    fun inferClass(x: Example): Coords {
        val logits = fullyConnected.invoke(x.matrix)

        var bestClass = Coords(0, 0)
        var bestLogit = logits[bestClass.row, bestClass.col]

        logits.forEachIndexedN { idx, logit ->
            if (logit > bestLogit) {
                bestClass = Coords(idx)
                bestLogit = logit
            }
        }
        return bestClass
    }

    override operator fun invoke(input: Matrix<Double>): Matrix<Double> {
        val logits = fullyConnected(input)
        val es = logits.map { Math.E.pow(it) }
        val sumEs = es.elementSum()
        return es.map { it / sumEs }
    }

    inner class MyBatchTrainer(val fullyConnectedTrainer: HiddenLayerBatchTrainer): OutputLayerBatchTrainer {
        private var training = true

        override fun train(
            input: Matrix<Double>,
            label: Coords
        ) {
            check(training)
            val ps = invoke(input)
            val dLossDLogit = Matrix(ps.numRows(), ps.numCols()) { row, col ->
                val p = ps[row, col]
                if (label.row == row && label.col == col) {
                    p - 1.0
                } else {
                    p
                }
            }
            fullyConnectedTrainer.train(input, dLossDLogit)
        }

        override fun updateParameters() {
            fullyConnectedTrainer.updateParameters()
            training = false
        }
    }

    override fun makeBatchTrainer(): OutputLayerBatchTrainer =
        MyBatchTrainer(fullyConnected.makeBatchTrainer())
}

private fun rand(shape: Shape): Matrix<Double> = rand(shape.numRows, shape.numCols)

private infix fun Matrix<Double>.dot(m: Matrix<Double>) = (this emul m).elementSum()

fun makeArrayOfMatrices(
    shape: Shape,
    fill: (row: Int, col: Int) -> Matrix<Double>
): NDArray<Matrix<Double>> {
    return DefaultGenericNDArray(shape.numRows, shape.numCols) { coords ->
        fill(coords[0], coords[1])
    }
}



