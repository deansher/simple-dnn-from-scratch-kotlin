/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package parts.wisdom.simplednn

import koma.extensions.*
import koma.internal.default.generated.ndarray.DefaultGenericNDArray
import koma.matrix.Matrix
import koma.ndarray.NDArray
import koma.rand
import koma.zeros
import kotlin.math.max
import kotlin.math.pow

private const val LEARNING_RATE = 5e-3
private const val MAX_INITIAL_VALUE = 1e-2

abstract class HiddenLayer(
    val inputShape: Shape,
    val outputShape: Shape
) {
    /**
     * Compute this `Layer`'s output from its input.
     */
    abstract operator fun invoke(modelInput: Matrix<Double>): Matrix<Double>

    /**
     * Make a stateful trainer that will process one batch of inputs.
     */
    abstract fun makeBatchTrainer(): HiddenLayerBatchTrainer
}

interface HiddenLayerBatchTrainer {
    // TODO: Refactor `train` to free up that name and use it for this function instead.
    fun train(
        modelInput: Matrix<Double>,
        dLossDOutput: Matrix<Double>
    )

    fun updateParameters()
}

abstract class Classifier {
    /**
     * Compute this `Layer`'s output from its input.
     */
    abstract operator fun invoke(input: Matrix<Double>): Matrix<Double>

    /**
     * Make a stateful trainer that will process one batch of inputs.
     */
    abstract fun makeBatchTrainer(): ClassifierBatchTrainer

    abstract fun inferClass(x: Example): Coords
}

interface ClassifierBatchTrainer {
    fun train(
        modelInput: Matrix<Double>,
        label: Coords
    )

    fun updateParameters()
}

class InputLayer(shape: Shape) : HiddenLayer(shape, shape) {
    override fun invoke(modelInput: Matrix<Double>): Matrix<Double> = modelInput

    override fun makeBatchTrainer(): HiddenLayerBatchTrainer {
        return object : HiddenLayerBatchTrainer {
            override fun train(modelInput: Matrix<Double>, dLossDOutput: Matrix<Double>) {
            }

            override fun updateParameters() {
            }
        }
    }

}

/**
 * A linear fully connected layer (no activation function).
 */
class FullyConnected(
    val source: HiddenLayer,
    outputShape: Shape
) : HiddenLayer(source.outputShape, outputShape) {
    /**
     * For each output, a weight for each input.
     */
    private var weight: NDArray<Matrix<Double>> =
        makeArrayOfMatrices(outputShape) { _, _ ->
            rand(inputShape.numRows, inputShape.numCols) * MAX_INITIAL_VALUE
        }

    /**
     * A bias for each output.
     */
    private var bias: Matrix<Double> = rand(outputShape) * MAX_INITIAL_VALUE

    override operator fun invoke(modelInput: Matrix<Double>): Matrix<Double> {
        val layerInput = source(modelInput)
        val weightedSums =
            Matrix(
                outputShape.numRows,
                outputShape.numCols
            ) { outputRow, outputCol ->
                (weight[outputRow, outputCol] dot layerInput).apply {
                    check(isFinite()) {
                        "dot product is $this"
                    }
                }
            }
        return weightedSums + bias
    }

    override fun makeBatchTrainer(): HiddenLayerBatchTrainer =
        object : HiddenLayerBatchTrainer {
            private var training = true
            private var sourceTrainer = source.makeBatchTrainer()

            /**
             * For each input, a weight for each output.
             */
            private var outputWeight: NDArray<Matrix<Double>> =
                makeArrayOfMatrices(inputShape) { inputRow, inputCol ->
                    Matrix(
                        outputShape.numRows,
                        outputShape.numCols
                    ) { outputRow, outputCol ->
                        weight[outputRow, outputCol][inputRow, inputCol]
                    }
                }

            private val batchDeltaBias =
                zeros(
                    outputShape.numRows,
                    outputShape.numCols
                )
            private val batchDeltaWeight: NDArray<Matrix<Double>> =
                makeArrayOfMatrices(outputShape) { _, _ ->
                    zeros(inputShape.numRows, inputShape.numCols)
                }

            override fun train(modelInput: Matrix<Double>, dLossDOutput: Matrix<Double>) {
                check(training)
                val layerInput = source(modelInput)

                dLossDOutput.forEachIndexedN { id, d ->
                    val deltaBias = -LEARNING_RATE * d
                    check(deltaBias.isFinite()) {
                        "example delta bias is $deltaBias"
                    }
                    batchDeltaBias[id[0], id[1]] =
                        (batchDeltaBias[id[0], id[1]] + deltaBias).apply {
                            check(isFinite()) {
                                "batch delta bias is $this"
                            }
                        }

                    val dw = batchDeltaWeight[id[0], id[1]]
                    for (row in 0 until inputShape.numRows) {
                        for (col in 0 until inputShape.numCols) {
                            val dLossDW = d * layerInput[row, col]
                            val deltaW = -LEARNING_RATE * dLossDW
                            check(deltaW.isFinite()) {
                                "example delta weight [$row, $col] is $deltaW"
                            }
                            dw[row, col] =
                                (dw[row, col] + deltaW).apply {
                                    check(isFinite()) {
                                        "batch delta weight [$row, $col] is $this"
                                    }
                                }
                        }
                    }
                }
                if (source !is InputLayer) {
                    val dLossDInput = Matrix(
                        inputShape.numRows,
                        inputShape.numCols
                    ) { inputRow, inputCol ->
                        dLossDOutput dot outputWeight[inputRow, inputCol]
                    }
                    sourceTrainer.train(modelInput, dLossDInput)
                }
            }

            override fun updateParameters() {
                training = false

                bias += batchDeltaBias
                weight.forEachIndexedN { iw, _ ->
                    weight[iw[0], iw[1]] += batchDeltaWeight[iw[0], iw[1]]
                }

                sourceTrainer.updateParameters()
            }
        }
}

class Relu(val source: HiddenLayer) : HiddenLayer(source.outputShape, source.outputShape) {
    override fun invoke(modelInput: Matrix<Double>): Matrix<Double> {
        val layerInput = source(modelInput)
        return Matrix(
            outputShape.numRows,
            outputShape.numCols
        ) { row, col ->
            val x = layerInput[row, col]
            check(x.isFinite()) {
                "layerInput[$row, $col] is $x"
            }
            max(0.0, x)
        }
    }

    override fun makeBatchTrainer(): HiddenLayerBatchTrainer =
        object : HiddenLayerBatchTrainer {
            private var sourceTrainer = source.makeBatchTrainer()

            override fun train(modelInput: Matrix<Double>, dLossDOutput: Matrix<Double>) {
                val layerInput = source(modelInput)
                val dLossDInput = Matrix(
                    outputShape.numRows,
                    outputShape.numCols
                ) { row, col ->
                    if (layerInput[row, col] > 0) {
                        dLossDOutput[row, col]
                    } else {
                        0.0
                    }
                }
                sourceTrainer.train(modelInput, dLossDInput)
            }

            override fun updateParameters() {
                sourceTrainer.updateParameters()
            }

        }
}

/**
 * Softmax classifier with cross-entropy loss.
 */
class Softmax(
    val source: HiddenLayer
) : Classifier() {

    override fun inferClass(x: Example): Coords {
        val logits = source.invoke(x.matrix)

        var bestClass = Coords(0, 0)
        var bestLogit = logits[bestClass.row, bestClass.col]

        logits.forEachIndexedN { idx, logit ->
            if (logit > bestLogit) {
                bestClass = Coords(idx)
                bestLogit = logit
            }
        }
        return bestClass
    }

    override operator fun invoke(input: Matrix<Double>): Matrix<Double> {
        val logits = source(input)
        val es = logits.map {
            Math.E.pow(it).apply {
                check(isFinite()) {
                    "logit of $it produced e^logit of $this"
                }
            }
        }
        val sumEs = es.elementSum()
        check(sumEs > 0.0) {
            "sum of e^logit under-flowed to 0.0"
        }
        check(sumEs.isFinite()) {
            "sum of e^logit is $sumEs"
        }
        return es.map {
            (it / sumEs).apply {
                check(isFinite()) {
                    "$it/$sumEs produces $this as probability"
                }
            }
        }
    }

    override fun makeBatchTrainer(): ClassifierBatchTrainer =
        object : ClassifierBatchTrainer {
            private val sourceTrainer = source.makeBatchTrainer()
            private var training = true

            override fun train(
                modelInput: Matrix<Double>,
                label: Coords
            ) {
                check(training)
                val ps = this@Softmax(modelInput)
                val dLossDLogit = Matrix(ps.numRows(), ps.numCols()) { row, col ->
                    val p = ps[row, col]
                    if (label.row == row && label.col == col) {
                        p - 1.0
                    } else {
                        p
                    }
                }
                sourceTrainer.train(modelInput, dLossDLogit)
            }

            override fun updateParameters() {
                training = false
                sourceTrainer.updateParameters()
            }
        }
}

private fun rand(shape: Shape): Matrix<Double> = rand(shape.numRows, shape.numCols)

private infix fun Matrix<Double>.dot(m: Matrix<Double>) = (this emul m).elementSum()

fun makeArrayOfMatrices(
    shape: Shape,
    fill: (row: Int, col: Int) -> Matrix<Double>
): NDArray<Matrix<Double>> {
    return DefaultGenericNDArray(shape.numRows, shape.numCols) { coords ->
        fill(coords[0], coords[1])
    }
}



