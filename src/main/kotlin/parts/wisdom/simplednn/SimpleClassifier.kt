/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package parts.wisdom.simplednn

import koma.extensions.emul
import koma.extensions.get
import koma.extensions.map
import koma.extensions.set
import koma.matrix.Matrix
import koma.rand
import koma.zeros
import kotlin.math.pow

private const val LEARNING_RATE = 3e-3
private const val MAX_INITIAL_VALUE = 1.0

/**
 * Classifier implemented as a single-layer neural net. The one layer is a fully connected softmax.
 * The loss is cross-entropy.
 */
class SimpleClassifier(
    private val exampleDims: ExampleDims,
    private val numClasses: Int
) {
    private var bias = rand(1, numClasses) * MAX_INITIAL_VALUE
    private var weight = MutableList(numClasses) {
        rand(exampleDims.numRows, exampleDims.numCols) * MAX_INITIAL_VALUE
    }

    fun inferClass(x: Example): Int {
        val logits = computeLogits(x)
        var bestClass = 0
        var bestLogit = logits[0, bestClass]
        for (c in 1 until numClasses) {
            if (logits[0, c] > bestLogit) {
                bestClass = c
                bestLogit = logits[0, c]
            }
        }
        return bestClass
    }

    private fun computeLogits(x: Example): Matrix<Double> {
        val weightedSums = Matrix(1, numClasses) { _, c ->
            weight[c] dot x.matrix
        }
        return weightedSums + bias
    }

    private fun computeProbabilities(x: Example): Matrix<Double> {
        val logits = computeLogits(x)
        val es = logits.map { Math.E.pow(it) }
        val sumEs = es.elementSum()
        return es.map { it / sumEs }
    }

    fun trainBatch(batch: List<Example>) {
        val batchDeltaBias = zeros(1, numClasses)
        val batchDeltaWeight = List(numClasses) {
            zeros(exampleDims.numRows, exampleDims.numCols)
        }
        for (x in batch) {
            val ps = computeProbabilities(x)
            for (c in 0 until numClasses) {
                val p = ps[0, c]
                val dLossDLogit =
                    if (c == x.label) {
                        p - 1.0
                    } else {
                        p
                    }
                val xDeltaBias = -LEARNING_RATE * dLossDLogit
                batchDeltaBias[0, c] += xDeltaBias

                val dw = batchDeltaWeight[c]
                for (row in 0 until exampleDims.numRows) {
                    for (col in 0 until exampleDims.numCols) {
                        val dLossDW = dLossDLogit * x.matrix[row, col]
                        val xDeltaW = -LEARNING_RATE * dLossDW
                        dw[row, col] += xDeltaW
                    }
                }
            }
        }
        bias += batchDeltaBias
        for (c in 0 until numClasses) {
            weight[c] += batchDeltaWeight[c]
        }
    }
}

private infix fun Matrix<Double>.dot(m: Matrix<Double>) = (this emul m).elementSum()

